{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c97b0a2-00f9-46fa-8e3d-661a6a5ef59d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Exercise Overview\n",
    "In this exercise we will play with Spark [Datasets & Dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes), some [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), and build a couple of binary classifiaction models using [Spark ML](https://spark.apache.org/docs/latest/ml-guide.html) (with some [MLlib](https://spark.apache.org/mllib/) too). \n",
    "<br><br>\n",
    "The set up and approach will not be too dissimilar to the standard type of approach you might do in [Sklearn](http://scikit-learn.org/stable/index.html). Spark has matured to the stage now where for 90% of what you need to do (when analysing tabular data) should be possible with Spark dataframes, SQL, and ML libraries. This is where this exercise is mainly trying to focus.  \n",
    "<br>\n",
    "Feel free to adapt this exercise to play with other datasets readily availabe in the Databricks enviornment (they are listed in a cell below). \n",
    "#####Getting Started\n",
    "To get started you will need to create and attach a databricks spark cluster to this notebook. This notebook was developed on a cluster created with: \n",
    "- Databricks Runtime Version 4.0 (includes Apache Spark 2.3.0, Scala 2.11)\n",
    "- Python Version 3\n",
    "\n",
    "#####Links & References\n",
    "\n",
    "Some useful links and references of sources used in creating this exercise:\n",
    "\n",
    "**Note**: Right click and open as new tab!\n",
    "<br>\n",
    "1. [Latest Spark Docs](https://spark.apache.org/docs/latest/index.html)\n",
    "1. [Databricks Homepage](https://databricks.com/)\n",
    "1. [Databricks Community Edition FAQ](https://databricks.com/product/faq/community-edition)\n",
    "1. [Databricks Self Paced Training](https://databricks.com/training-overview/training-self-paced)\n",
    "1. [Databricks Notebook Guide](https://docs.databricks.com/user-guide/notebooks/index.html)\n",
    "1. [Databricks Binary Classification Tutorial](https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html#binary-classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695a1fed-7ad1-4a91-bd70-46e0275a793e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Get Data\n",
    "\n",
    "Here we will pull in some sample data that is already pre-loaded onto all databricks clusters.\n",
    "\n",
    "Feel free to adapt this notebook later to play around with a different dataset if you like (all available are listed in a cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e9b324-2b85-41ea-b2a9-cc7a9aa97fa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532468253000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>1696350956201</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>1696350956202</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>1696350956202</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1532468253000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455043490000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         1696350956201
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         1696350956202
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         1696350956202
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display datasets already in databricks\n",
    "display(dbutils.fs.ls(\"/databricks-datasets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7388cb6-bf4f-44d6-a7cd-50d35e20a22d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Lets take a look at the '**adult**' dataset on the filesystem. This is the typical US Census data you often see online in tutorials. [Here](https://archive.ics.uci.edu/ml/datasets/adult) is the same data in the UCI repository.\n",
    "\n",
    "_As an aside: [here](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census) this same dataset is used as a quickstart example for Google CLoud ML & Tensorflow Estimator API (in case youd be interested in playing with tensorflow on the same dataset as here)._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db143421-89ed-4e9b-9272-0ddf8f2a4fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/adult/adult.data</td><td>adult.data</td><td>3974305</td><td>1444260537000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/adult/adult.data",
         "adult.data",
         3974305,
         1444260537000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls databricks-datasets/adult/adult.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df40add-37cc-47ff-adc1-3eb0852a5afc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Note**: Above  %fs is just some file system cell magic that is specific to databricks. More info [here](https://docs.databricks.com/user-guide/notebooks/index.html#mix-languages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d3180c-0f4b-4375-8f97-7975ca99ef77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Spark SQL\n",
    "Below we will use Spark SQL to load in the data and then register it as a Dataframe aswell. So the end result will be a Spark SQL table called _adult_ and a Spark Dataframe called _df_adult_. \n",
    "<br><br>\n",
    "This is an example of the flexibility in Spark in that you could do lots of you ETL and data wrangling using either Spark SQL or Dataframes and pyspark. Most of the time it's a case of using whatever you are most comfortable with.\n",
    "<br><br>\n",
    "When you get more advanced then you might looking the pro's and con's of each and when you might favour one or the other (or operating direclty on RDD's), [here](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html) is a good article on the issues. For now, no need to overthink it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ab6a7a-748b-49a7-a1dc-afdccd2695c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "-- drop the table if it already exists\n",
    "DROP TABLE IF EXISTS adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22f5e37-a6e4-4d5c-863e-61e9fda375f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- create a new table in Spark SQL from the datasets already loaded in the underlying filesystem.\n",
    "-- In the real world you might be pointing at a file on HDFS or a hive table etc. \n",
    "CREATE TABLE adult (\n",
    "  age DOUBLE,\n",
    "  workclass STRING,\n",
    "  fnlwgt DOUBLE,\n",
    "  education STRING,\n",
    "  education_num DOUBLE,\n",
    "  marital_status STRING,\n",
    "  occupation STRING,\n",
    "  relationship STRING,\n",
    "  race STRING,\n",
    "  sex STRING,\n",
    "  capital_gain DOUBLE,\n",
    "  capital_loss DOUBLE,\n",
    "  hours_per_week DOUBLE,\n",
    "  native_country STRING,\n",
    "  income STRING)\n",
    "USING com.databricks.spark.csv\n",
    "OPTIONS (path \"/databricks-datasets/adult/adult.data\", header \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73f000c-7068-4d9c-a559-18dc7bda95f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>workclass</th><th>fnlwgt</th><th>education</th><th>education_num</th><th>marital_status</th><th>occupation</th><th>relationship</th><th>race</th><th>sex</th><th>capital_gain</th><th>capital_loss</th><th>hours_per_week</th><th>native_country</th><th>income</th></tr></thead><tbody><tr><td>50.0</td><td> Self-emp-not-inc</td><td>83311.0</td><td> Bachelors</td><td>13.0</td><td> Married-civ-spouse</td><td> Exec-managerial</td><td> Husband</td><td> White</td><td> Male</td><td>0.0</td><td>0.0</td><td>13.0</td><td> United-States</td><td> <=50K</td></tr><tr><td>38.0</td><td> Private</td><td>215646.0</td><td> HS-grad</td><td>9.0</td><td> Divorced</td><td> Handlers-cleaners</td><td> Not-in-family</td><td> White</td><td> Male</td><td>0.0</td><td>0.0</td><td>40.0</td><td> United-States</td><td> <=50K</td></tr><tr><td>53.0</td><td> Private</td><td>234721.0</td><td> 11th</td><td>7.0</td><td> Married-civ-spouse</td><td> Handlers-cleaners</td><td> Husband</td><td> Black</td><td> Male</td><td>0.0</td><td>0.0</td><td>40.0</td><td> United-States</td><td> <=50K</td></tr><tr><td>28.0</td><td> Private</td><td>338409.0</td><td> Bachelors</td><td>13.0</td><td> Married-civ-spouse</td><td> Prof-specialty</td><td> Wife</td><td> Black</td><td> Female</td><td>0.0</td><td>0.0</td><td>40.0</td><td> Cuba</td><td> <=50K</td></tr><tr><td>37.0</td><td> Private</td><td>284582.0</td><td> Masters</td><td>14.0</td><td> Married-civ-spouse</td><td> Exec-managerial</td><td> Wife</td><td> White</td><td> Female</td><td>0.0</td><td>0.0</td><td>40.0</td><td> United-States</td><td> <=50K</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         50.0,
         " Self-emp-not-inc",
         83311.0,
         " Bachelors",
         13.0,
         " Married-civ-spouse",
         " Exec-managerial",
         " Husband",
         " White",
         " Male",
         0.0,
         0.0,
         13.0,
         " United-States",
         " <=50K"
        ],
        [
         38.0,
         " Private",
         215646.0,
         " HS-grad",
         9.0,
         " Divorced",
         " Handlers-cleaners",
         " Not-in-family",
         " White",
         " Male",
         0.0,
         0.0,
         40.0,
         " United-States",
         " <=50K"
        ],
        [
         53.0,
         " Private",
         234721.0,
         " 11th",
         7.0,
         " Married-civ-spouse",
         " Handlers-cleaners",
         " Husband",
         " Black",
         " Male",
         0.0,
         0.0,
         40.0,
         " United-States",
         " <=50K"
        ],
        [
         28.0,
         " Private",
         338409.0,
         " Bachelors",
         13.0,
         " Married-civ-spouse",
         " Prof-specialty",
         " Wife",
         " Black",
         " Female",
         0.0,
         0.0,
         40.0,
         " Cuba",
         " <=50K"
        ],
        [
         37.0,
         " Private",
         284582.0,
         " Masters",
         14.0,
         " Married-civ-spouse",
         " Exec-managerial",
         " Wife",
         " White",
         " Female",
         0.0,
         0.0,
         40.0,
         " United-States",
         " <=50K"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "workclass",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fnlwgt",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "education",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "education_num",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "marital_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "occupation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "relationship",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "race",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sex",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "capital_gain",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "capital_loss",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "hours_per_week",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "native_country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "income",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the data\n",
    "#spark.sql(\"SELECT * FROM adult LIMIT 5\").show() \n",
    "# this will look prettier in Databricks if you use display() instead\n",
    "display(spark.sql(\"SELECT * FROM adult LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41c5e9-3101-41ba-a7e9-9e84399f3150",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you are more comfortable with SQL then as you can see below, its very easy to just get going with writing standard SQL type code to analyse your data, do data wrangling and create new dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1219e3d2-a3ff-4d81-b688-c6751f950822",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>occupation</th><th>n</th><th>married_rate</th><th>widow_rate</th><th>divorce_rate</th><th>separated_rate</th><th>bachelor_rate</th></tr></thead><tbody><tr><td> Prof-specialty</td><td>4140</td><td>0.53</td><td>0.02</td><td>0.13</td><td>0.02</td><td>0.3</td></tr><tr><td> Craft-repair</td><td>4099</td><td>0.64</td><td>0.01</td><td>0.11</td><td>0.03</td><td>0.21</td></tr><tr><td> Exec-managerial</td><td>4066</td><td>0.61</td><td>0.02</td><td>0.15</td><td>0.02</td><td>0.2</td></tr><tr><td> Adm-clerical</td><td>3769</td><td>0.28</td><td>0.04</td><td>0.22</td><td>0.04</td><td>0.42</td></tr><tr><td> Sales</td><td>3650</td><td>0.47</td><td>0.03</td><td>0.12</td><td>0.03</td><td>0.36</td></tr><tr><td> Other-service</td><td>3295</td><td>0.24</td><td>0.05</td><td>0.15</td><td>0.06</td><td>0.5</td></tr><tr><td> Machine-op-inspct</td><td>2002</td><td>0.51</td><td>0.03</td><td>0.14</td><td>0.04</td><td>0.29</td></tr><tr><td> ?</td><td>1843</td><td>0.36</td><td>0.08</td><td>0.1</td><td>0.04</td><td>0.42</td></tr><tr><td> Transport-moving</td><td>1597</td><td>0.63</td><td>0.02</td><td>0.11</td><td>0.02</td><td>0.21</td></tr><tr><td> Handlers-cleaners</td><td>1370</td><td>0.36</td><td>0.01</td><td>0.09</td><td>0.03</td><td>0.51</td></tr><tr><td> Farming-fishing</td><td>994</td><td>0.6</td><td>0.02</td><td>0.06</td><td>0.02</td><td>0.29</td></tr><tr><td> Tech-support</td><td>928</td><td>0.44</td><td>0.02</td><td>0.15</td><td>0.03</td><td>0.36</td></tr><tr><td> Protective-serv</td><td>649</td><td>0.6</td><td>0.01</td><td>0.12</td><td>0.02</td><td>0.24</td></tr><tr><td> Priv-house-serv</td><td>149</td><td>0.13</td><td>0.15</td><td>0.19</td><td>0.08</td><td>0.45</td></tr><tr><td> Armed-Forces</td><td>9</td><td>0.33</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.67</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         " Prof-specialty",
         4140,
         0.53,
         0.02,
         0.13,
         0.02,
         0.3
        ],
        [
         " Craft-repair",
         4099,
         0.64,
         0.01,
         0.11,
         0.03,
         0.21
        ],
        [
         " Exec-managerial",
         4066,
         0.61,
         0.02,
         0.15,
         0.02,
         0.2
        ],
        [
         " Adm-clerical",
         3769,
         0.28,
         0.04,
         0.22,
         0.04,
         0.42
        ],
        [
         " Sales",
         3650,
         0.47,
         0.03,
         0.12,
         0.03,
         0.36
        ],
        [
         " Other-service",
         3295,
         0.24,
         0.05,
         0.15,
         0.06,
         0.5
        ],
        [
         " Machine-op-inspct",
         2002,
         0.51,
         0.03,
         0.14,
         0.04,
         0.29
        ],
        [
         " ?",
         1843,
         0.36,
         0.08,
         0.1,
         0.04,
         0.42
        ],
        [
         " Transport-moving",
         1597,
         0.63,
         0.02,
         0.11,
         0.02,
         0.21
        ],
        [
         " Handlers-cleaners",
         1370,
         0.36,
         0.01,
         0.09,
         0.03,
         0.51
        ],
        [
         " Farming-fishing",
         994,
         0.6,
         0.02,
         0.06,
         0.02,
         0.29
        ],
        [
         " Tech-support",
         928,
         0.44,
         0.02,
         0.15,
         0.03,
         0.36
        ],
        [
         " Protective-serv",
         649,
         0.6,
         0.01,
         0.12,
         0.02,
         0.24
        ],
        [
         " Priv-house-serv",
         149,
         0.13,
         0.15,
         0.19,
         0.08,
         0.45
        ],
        [
         " Armed-Forces",
         9,
         0.33,
         0.0,
         0.0,
         0.0,
         0.67
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "occupation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "married_rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "widow_rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "divorce_rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "separated_rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "bachelor_rate",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets get some summary marital status rates by occupation\n",
    "result = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "    occupation,\n",
    "    SUM(1) as n,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) LIKE 'Married-%',1,0)),2) as married_rate,\n",
    "    ROUND(AVG(if(lower(marital_status) LIKE '%widow%',1,0)),2) as widow_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Divorced',1,0)),2) as divorce_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Separated',1,0)),2) as separated_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Never-married',1,0)),2) as bachelor_rate\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY n DESC\n",
    "  \"\"\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce745db8-1b57-47cd-976b-4ce3ee440604",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can easily register dataframes as a table for Spark SQL too. So this way you can easily move between Dataframes and Spark SQL for whatever reason.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9007db35-22d8-4432-bcea-9bbb90017fa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+------------+----------+------------+--------------+-------------+\n|      occupation|   n|married_rate|widow_rate|divorce_rate|separated_rate|bachelor_rate|\n+----------------+----+------------+----------+------------+--------------+-------------+\n|  Prof-specialty|4140|        0.53|      0.02|        0.13|          0.02|          0.3|\n|    Craft-repair|4099|        0.64|      0.01|        0.11|          0.03|         0.21|\n| Exec-managerial|4066|        0.61|      0.02|        0.15|          0.02|          0.2|\n|    Adm-clerical|3769|        0.28|      0.04|        0.22|          0.04|         0.42|\n|           Sales|3650|        0.47|      0.03|        0.12|          0.03|         0.36|\n+----------------+----+------------+----------+------------+--------------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# register the df we just made as a table for spark sql\n",
    "sqlContext.registerDataFrameAsTable(result, \"result\")\n",
    "spark.sql(\"SELECT * FROM result\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24bbcd7e-55ec-4929-a898-09a27acf9269",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 1</span>\n",
    "\n",
    "1. Write some spark sql to get the top 'bachelor_rate' by 'education' group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f44ee66-3975-4d09-86a9-6fe1d0783e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------+\n|education|  n|bachelor_rate|\n+---------+---+-------------+\n|     12th|433|         0.54|\n+---------+---+-------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "result1 = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT\n",
    "    education,\n",
    "    count(*) as n,\n",
    "    round(avg(if(LTRIM(marital_status) == 'Never-married',1,0)), 2) as bachelor_rate\n",
    "  FROM\n",
    "    adult\n",
    "  GROUP BY education\n",
    "  ORDER BY bachelor_rate DESC\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "result1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964b6472-2d89-4158-b517-0955f1e3c587",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------+\n|education|  n|bachelor_rate|\n+---------+---+-------------+\n|     12th|433|         0.54|\n+---------+---+-------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "### Question 1.1 Answer ###\n",
    "\n",
    "result = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT\n",
    "    education,\n",
    "    SUM(1) as n,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Never-married',1,0)),2) as bachelor_rate\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY bachelor_rate DESC\n",
    "  \"\"\")\n",
    "\n",
    "result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82050620-e0ac-45a1-96ac-11ddce045cb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Spark DataFrames\n",
    "Below we will create our DataFrame from the SQL table and do some similar analysis as we did with Spark SQL but using the DataFrames API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbad390a-677d-4b3b-8e01-9dd7bb211ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register a df from the sql df\n",
    "df_adult = spark.table(\"adult\")\n",
    "cols = df_adult.columns # this will be used much later in the notebook, ignore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da26c930-94a3-4289-ab00-61f5b283f9c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- age: double (nullable = true)\n |-- workclass: string (nullable = true)\n |-- fnlwgt: double (nullable = true)\n |-- education: string (nullable = true)\n |-- education_num: double (nullable = true)\n |-- marital_status: string (nullable = true)\n |-- occupation: string (nullable = true)\n |-- relationship: string (nullable = true)\n |-- race: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- capital_gain: double (nullable = true)\n |-- capital_loss: double (nullable = true)\n |-- hours_per_week: double (nullable = true)\n |-- native_country: string (nullable = true)\n |-- income: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# look at df schema\n",
    "df_adult.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0443d5cc-85c8-462c-8998-0334ac36d6c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# look at the df\n",
    "display(df_adult)\n",
    "#df_adult.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f3c980-fda5-4a86-8a83-75846e041b74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below we will do a similar calculation to what we did above but using the DataFrames API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460c47b6-0405-4ce6-8015-39625fbc514a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n|      occupation|divorced_rate|\n+----------------+-------------+\n|    Adm-clerical|         0.22|\n| Priv-house-serv|         0.19|\n| Exec-managerial|         0.15|\n|    Tech-support|         0.15|\n|   Other-service|         0.15|\n+----------------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# import what we will need\n",
    "from pyspark.sql.functions import when, col, mean, desc, round\n",
    "\n",
    "# wrangle the data a bit\n",
    "df_result = df_adult.select(\n",
    "  df_adult['occupation'], \n",
    "  # create a 1/0 type col on the fly\n",
    "  when( col('marital_status') == ' Divorced' , 1 ).otherwise(0).alias('is_divorced')\n",
    ")\n",
    "# do grouping (and a round)\n",
    "df_result = df_result.groupBy('occupation').agg(round(mean('is_divorced'),2).alias('divorced_rate'))\n",
    "# do ordering\n",
    "df_result = df_result.orderBy(desc('divorced_rate'))\n",
    "# show results\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e688a7f-a03c-4c74-94ae-42d999aca3bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you can see the dataframes api is a bit more verbose then just expressing what you want to do in standard SQL.<br><br>But some prefer it and might be more used to it, and there could be cases where expressing what you need to do might just be better using the DataFrame API if it is too complicated for a simple SQL expression for example of maybe involves recursion of some type.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252f073d-4272-4703-886a-4b37c9cd47f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 2</span>\n",
    "1. Write some pyspark to get the top 'bachelor_rate' by 'education' group using DataFrame operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7edc31-2774-485e-9f9c-9064545f070c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n|education|bachelor_rate|\n+---------+-------------+\n|     12th|         0.54|\n+---------+-------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "df_result2 = df_adult.select(\n",
    "  df_adult['education'],\n",
    "  when(col('marital_status') == ' Never-married', 1).otherwise(0).alias('is_bachelor')\n",
    ")\n",
    "\n",
    "df_result2 = df_result2.groupBy('education').agg(round(mean('is_bachelor'), 2).alias('bachelor_rate'))\n",
    "\n",
    "df_result2 = df_result2.orderBy(desc('bachelor_rate'))\n",
    "\n",
    "df_result2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5c2957-a783-47ec-bcfd-a769ba0cf294",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n|education|bachelor_rate|\n+---------+-------------+\n|     12th|         0.54|\n+---------+-------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "### Question 2.1 Answer ###\n",
    "\n",
    "# wrangle the data a bit\n",
    "df_result = df_adult.select(\n",
    "  df_adult['education'], \n",
    "  # create a 1/0 type col on the fly\n",
    "  when( col('marital_status') == ' Never-married' , 1 ).otherwise(0).alias('is_bachelor')\n",
    ")\n",
    "# do grouping (and a round)\n",
    "df_result = df_result.groupBy('education').agg(round(mean('is_bachelor'),2).alias('bachelor_rate'))\n",
    "# do ordering\n",
    "df_result = df_result.orderBy(desc('bachelor_rate'))\n",
    "\n",
    "df_result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f244c2-4b2e-4727-8c65-66cbd75a69f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Explore & Visualize Data\n",
    "It's very easy to [collect()](https://spark.apache.org/docs/latest/rdd-programming-guide.html#printing-elements-of-an-rdd) your Spark DataFrame data into a Pandas df and then continue to analyse or plot as you might normally.\n",
    "<br><br>\n",
    "Obviously if you try to collect() a huge DataFrame then you will run into issues, so usually you would only collect aggregated or sampled data into a Pandas df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7640afab-8c43-46e2-91c6-43b5834e1a23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         occupation  plus_50k\n0   Exec-managerial  0.484014\n1    Prof-specialty  0.449034\n2   Protective-serv  0.325116\n3      Tech-support  0.304957\n4             Sales  0.269315\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# do some analysis\n",
    "result = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "    occupation,\n",
    "    AVG(IF(income = ' >50K',1,0)) as plus_50k\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY 2 DESC\n",
    "  \"\"\")\n",
    "\n",
    "# collect results into a pandas df\n",
    "df_pandas = pd.DataFrame(\n",
    "  result.collect(),\n",
    "  columns=result.schema.names\n",
    ")\n",
    "\n",
    "# look at df\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7e5b12-632a-4dd9-b66b-6fb03ea902d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        plus_50k\ncount  15.000000\nmean    0.197357\nstd     0.143993\nmin     0.006711\n25%     0.107373\n50%     0.134518\n75%     0.287136\nmax     0.484014\n"
     ]
    }
   ],
   "source": [
    "print(df_pandas.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71f0594-35e9-480b-b3c9-7793765e3af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 15 entries, 0 to 14\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   occupation  15 non-null     object \n 1   plus_50k    15 non-null     float64\ndtypes: float64(1), object(1)\nmemory usage: 368.0+ bytes\nNone\n"
     ]
    }
   ],
   "source": [
    "print(df_pandas.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1090b4c-4789-4b83-84c7-6150b8158026",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we will just do some very basic plotting to show how you might collect what you are interested in into a Pandas DF and then just plot any way you normally would.\n",
    "\n",
    "For simplicity we are going to use the plotting functionality built into pandas (you could make this a pretty as you want). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946501d8-b915-4fbd-b9cb-a0a31a802078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# i like ggplot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# get simple plot on the pandas data\n",
    "myplot = df_pandas.plot(kind='barh', x='occupation', y='plus_50k')\n",
    "\n",
    "# display the plot (note - display() is a databricks function - \n",
    "# more info on plotting in Databricks is here: https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot.html)\n",
    "display(myplot.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9aafee-4f84-4f93-96ac-70a73bc2a588",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can also easily get summary stats on a Spark DataFrame like below. [Here](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html) is a nice blog post that has more examples.<br><br>So this is an example of why you might want to move from Spark SQL into DataFrames API as being able to just call describe() on the Spark DF is easier then trying to do the equivilant in Spark SQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2fa9c46-ff91-4424-ad28-9dd4f74f078e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n|summary|               age|     education_num|\n+-------+------------------+------------------+\n|  count|             32560|             32560|\n|   mean|38.581633906633904| 10.08058968058968|\n| stddev|13.640641827464002|2.5727089681052058|\n|    min|              17.0|               1.0|\n|    max|              90.0|              16.0|\n+-------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# describe df\n",
    "df_adult.select(df_adult['age'],df_adult['education_num']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72eff44a-2ff4-417a-9bea-a44ebec44bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|income|\n+------+\n| <=50K|\n| <=50K|\n| <=50K|\n| <=50K|\n| <=50K|\n| <=50K|\n|  >50K|\n|  >50K|\n|  >50K|\n|  >50K|\n+------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_adult.select(['income']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7bfbde-68e3-47a3-9f3e-278bce83808b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ML Pipeline - Logistic Regression vs Random Forest\n",
    "\n",
    "Below we will create two [Spark ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) - one that fits a logistic regression and one that fits a random forest. We will then compare the performance of each.\n",
    "\n",
    "**Note**: A lot of the code below is adapted from [this example](https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a32ba3-ff2c-4ca0-91ca-8e6187f24363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2657356-694c-4a30-80bd-02ede201a55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2fa539-5109-4b69-bc89-e7a7439a1469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96a56cc-57f4-4c4b-856e-f2e5701e95fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(df_adult)\n",
    "dataset = pipelineModel.transform(df_adult)\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5e74a5-dff1-45a3-9155-b9944d95e9aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8de9b08-6ff6-4b6a-b67f-a78f2db52ccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate in the training data is 0.2398931277648811\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# get the rate of the positive outcome from the training data to use as a threshold in the model\n",
    "training_data_positive_rate = trainingData.select(avg(trainingData['label'])).collect()[0][0] \n",
    "\n",
    "print(\"Positive rate in the training data is {}\".format(training_data_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb85c94-76bc-42aa-82b4-d00e6369e6bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Logistic Regression - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e5691c-1522-4b7e-a914-57b1bfa7a3a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold based on model performance on training data is 0.3178139130754516\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# set threshold for the probability above which to predict a 1\n",
    "lr.setThreshold(training_data_positive_rate)\n",
    "# lr.setThreshold(0.5) # could use this if knew you had balanced data\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# get training summary used for eval metrics and other params\n",
    "lrTrainingSummary = lrModel.summary\n",
    "\n",
    "# Find the best model threshold if you would like to use that instead of the empirical positve rate\n",
    "fMeasure = lrTrainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "lrBestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "  \n",
    "print(\"Best threshold based on model performance on training data is {}\".format(lrBestThreshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f512e9-7d60-427d-8f17-530e08639605",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####GBM - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b1128b-d223-4d3d-b202-5ad887e9cda3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 3</span>\n",
    "1. Train a GBTClassifier on the training data, call the trained model 'gbModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87612b85-b839-4782-a741-ba3a7a35a3be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbModel = GBTClassifier(labelCol = 'label', featuresCol ='features', maxIter = 10)\n",
    "\n",
    "gbModel = gbModel.fit(trainingData)\n",
    "\n",
    "#gbModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998d8fa4-1162-4f46-bd6a-40d05beadf31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Question 3.1 Answer ###\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "gb2 = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "gbModel2 = gb2.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52bee49-0815-4252-a35f-d6a9b8b52be2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Logistic Regression - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb54547-1466-46ac-81bc-a9ae5d32dc5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "lrPredictions = lrModel.transform(testData)\n",
    "\n",
    "# display predictions\n",
    "display(lrPredictions.select(\"label\", \"prediction\", \"probability\"))\n",
    "#display(lrPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b53bb9-d298-4af9-bdc8-fd94a5f0b388",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###GBM - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2f8c8c-070a-466c-a315-d26562880c66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 4</span>\n",
    "1. Get predictions on the test data for your GBTClassifier. Call the predictions df 'gbPredictions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625d8e5a-64c0-41d5-bd31-34895b5977a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gb_pred = gbModel.transform(testData)\n",
    "\n",
    "display(gb_pred.select(['label', 'prediction', 'probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88af5d4c-6bc4-45c0-a4d5-39656e5aea3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Question 4.1 Answer ###\n",
    "\n",
    "# make predictions on test data\n",
    "gbPredictions = gbModel2.transform(testData)\n",
    "\n",
    "display(gbPredictions.select(\"label\", \"prediction\", \"probability\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16eabc78-ad9f-4a58-a66d-b8bdc6669417",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Logistic Regression - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3194131-7989-4854-b39a-622c6ef6f2cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 5</span>\n",
    "1. Complete the print_performance_metrics() function below to also include measures of F1, Precision, Recall, False Positive Rate and True Positive Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20e5588-1c65-428e-b125-9f2cb6ed8a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "  aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "  print(\"auc = {}\".format(auc))\n",
    "  print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "  # get rdd of predictions and labels for mllib eval metrics\n",
    "  predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "  # Instantiate metrics objects\n",
    "  binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "  multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  # Area under precision-recall curve\n",
    "  print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "  # Area under ROC curve\n",
    "  print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "  # Accuracy\n",
    "  print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "  # Confusion Matrix\n",
    "  print(multi_metrics.confusionMatrix())\n",
    "  \n",
    "  ### Question 5.1 Answer ###\n",
    "  \n",
    "  # F1\n",
    "  print(\"F1 of label 0 = {}\".format(multi_metrics.fMeasure(0.0)))\n",
    "  print(\"F1 of label 1 = {}\".format(multi_metrics.fMeasure(1.0)))\n",
    "  # Precision\n",
    "  print(\"Precision of label 0 = {}\".format(multi_metrics.precision(0.0)))\n",
    "  print(\"Precision of label 1 = {}\".format(multi_metrics.precision(1.0)))\n",
    "  # Recall\n",
    "  print(\"Recall of label 0 = {}\".format(multi_metrics.recall(0.0)))\n",
    "  print(\"Recall of label 1 = {}\".format(multi_metrics.recall(1.0)))\n",
    "  # FPR\n",
    "  print(\"FPR of label 0 = {}\".format(multi_metrics.falsePositiveRate(0.0)))\n",
    "  print(\"FPR of label 1 = {}\".format(multi_metrics.falsePositiveRate(1.0)))\n",
    "  # TPR\n",
    "  print(\"TPR of label 0 = {}\".format(multi_metrics.truePositiveRate(0.0)))\n",
    "  print(\"TPR of label 1 = {}\".format(multi_metrics.truePositiveRate(1.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2b2686-3efe-4ced-8408-282bd7b9e25c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.9022382581905842\naupr = 0.7639327846135503\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:166: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.550904296478963\nArea under ROC = 0.8214764233357801\nAccuracy = 0.8119025593586185\nDenseMatrix([[5913., 1452.],\n             [ 378., 1986.]])\nF1 of label 0 = 0.8659929701230228\nF1 of label 1 = 0.6845915201654602\nPrecision of label 0 = 0.9399141630901288\nPrecision of label 1 = 0.5776614310645725\nRecall of label 0 = 0.8028513238289205\nRecall of label 1 = 0.8401015228426396\nFPR of label 0 = 0.1598984771573604\nFPR of label 1 = 0.19714867617107942\nTPR of label 0 = 0.8028513238289205\nTPR of label 1 = 0.8401015228426396\n"
     ]
    }
   ],
   "source": [
    "print_performance_metrics(lrPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f52961-1e3b-43b2-b2d9-02d35d9cf308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(prediction=1.0, label=0.0),\n",
       " Row(prediction=1.0, label=0.0),\n",
       " Row(prediction=1.0, label=0.0),\n",
       " Row(prediction=1.0, label=0.0),\n",
       " Row(prediction=1.0, label=0.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionAndLabels = lrPredictions.select(\"prediction\",\"label\").rdd\n",
    "predictionAndLabels.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21aa0bfe-9510-470d-9692-7c252c9bc58f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####GBM - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111e82b8-8470-42d0-9485-5fdb59912e56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.9067860806416225\naupr = 0.7811867862797953\nArea under PR = 0.6395496885361467\nArea under ROC = 0.7432483518907165\nAccuracy = 0.8465412683729058\nDenseMatrix([[6954.,  411.],\n             [1082., 1282.]])\nF1 of label 0 = 0.9030582429712356\nF1 of label 1 = 0.6319940842987429\nPrecision of label 0 = 0.8653558984569437\nPrecision of label 1 = 0.757235676314235\nRecall of label 0 = 0.9441955193482688\nRecall of label 1 = 0.5423011844331641\nFPR of label 0 = 0.4576988155668359\nFPR of label 1 = 0.05580448065173116\nTPR of label 0 = 0.9441955193482688\nTPR of label 1 = 0.5423011844331641\n"
     ]
    }
   ],
   "source": [
    "print_performance_metrics(gbPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d7ac8b-69d9-47d4-9412-9342faad7c05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "766d216e-da4c-4d99-8693-44c74ef75370",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For each model you can run the below comand to see its params and a brief explanation of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfef6fc-f469-4556-bebc-9ed17b605b8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\nmaxIter: max number of iterations (>= 0). (default: 100, current: 10)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5, current: 0.2398931277648811)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077280d3-39ad-4b29-b97e-ca45993c7527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\nfeaturesCol: features column name. (default: features, current: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\nlabelCol: label column name. (default: label, current: label)\nleafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\nlossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\nmaxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\nmaxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\nmaxIter: max number of iterations (>= 0). (default: 20, current: 10)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\nminWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nseed: random seed. (default: 3504127614838123891)\nstepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\nvalidationIndicatorCol: name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation. (undefined)\nvalidationTol: Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used. (default: 0.01)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(gb2.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5daa3f89-6335-47e0-96e6-b1e0c43a3aea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Logisitic Regression - Param Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0ad441-c23a-484b-81e4-14b7fbf0b27e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "lrParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [2, 5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72cc844-4efe-498a-aff0-18403c629de6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####GBM - Param Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47696aef-d15e-4885-94aa-e263c7a56121",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 6</span>\n",
    "1. Build out a param grid for the gb model, call it 'gbParamGrid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5cbe95-f1dd-49cd-a1e2-4b96a87a67d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bgParamGrid = (ParamGridBuilder()\n",
    "               .addGrid(gb2.maxDepth, [2, 3, 4])\n",
    "               .addGrid(gb2.maxIter, [5, 10])\n",
    "               .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b95f49-81f1-4edc-949d-e453c6214d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Question 6.1 Answer ###\n",
    "import numpy as np\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "gbParamGrid = (ParamGridBuilder()\n",
    "#             .addGrid(gb.maxDepth, np.arange(2, 10))\n",
    "#             .addGrid(gb.maxIter, [50])\n",
    "             .addGrid(gb2.maxDepth, np.arange(2, 5))\n",
    "             .addGrid(gb2.maxIter, [10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7eecc2-5510-4563-8fc1-ec0f8efa7ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Logistic Regression - Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2856c2-cec9-43e5-8b42-61f93429b97f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set up an evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# Create CrossValidator\n",
    "lrCv = CrossValidator(estimator=lr, estimatorParamMaps=lrParamGrid, evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "lrCvModel = lrCv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919f51ca-6b9d-43ac-a32b-16bf811f3a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n0.0\n5\n"
     ]
    }
   ],
   "source": [
    "# below approach to getting at the best params from the best cv model taken from:\n",
    "# https://stackoverflow.com/a/46353730/1919374\n",
    "\n",
    "# look at best params from the CV\n",
    "print(lrCvModel.bestModel._java_obj.getRegParam())\n",
    "print(lrCvModel.bestModel._java_obj.getElasticNetParam())\n",
    "print(lrCvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea6bd5d5-df8c-4eaa-aa24-7dcc2e7627cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####GBM - Perform Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb2030b9-f96b-4402-b6cf-ab05effc3944",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 7</span>\n",
    "1. Perform cross validation of params on your 'gb' model.\n",
    "1. Print out the best params you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a9e5e6-449d-4f6e-8fee-3fc3e1e56c2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Question 7.1 Answer ###\n",
    "\n",
    "# Create CrossValidator\n",
    "gbCv = CrossValidator(estimator=gb2, estimatorParamMaps=gbParamGrid, evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "gbCvModel = gbCv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e1eff8-e84e-4a8b-bab1-3b5ce2e5e9f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n10\n"
     ]
    }
   ],
   "source": [
    "### Question 7.2 Answer ###\n",
    "\n",
    "# look at best params from the CV\n",
    "print(gbCvModel.bestModel._java_obj.getMaxDepth())\n",
    "print(gbCvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efd7e902-4c5c-48ca-8dfc-93a8ca1192e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Logistic Regression - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402209de-a5da-4195-ac8a-fe20fb1c9b87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "lrCvPredictions = lrCvModel.transform(testData)\n",
    "\n",
    "display(lrCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff1a654-95f5-4e08-9705-76e133141de7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####GBM - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e0cddb-99ed-4fe9-9c46-ed9dc12f5044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbCvPredictions = gbCvModel.transform(testData)\n",
    "\n",
    "display(gbCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9db16de-2dcd-45ee-9e00-50009530a361",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Logistic Regression - CV Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e58e24-4d40-4c50-bbb0-73e059994495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.8970633845772201\naupr = 0.7460869514036187\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:166: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.5330158297497791\nArea under ROC = 0.8134734010841509\nAccuracy = 0.799568300955905\nDenseMatrix([[5792., 1573.],\n             [ 377., 1987.]])\nF1 of label 0 = 0.8559184276636619\nF1 of label 1 = 0.6708305199189737\nPrecision of label 0 = 0.9388879883287404\nPrecision of label 1 = 0.5581460674157304\nRecall of label 0 = 0.7864222674813306\nRecall of label 1 = 0.8405245346869712\nFPR of label 0 = 0.15947546531302875\nFPR of label 1 = 0.21357773251866938\nTPR of label 0 = 0.7864222674813306\nTPR of label 1 = 0.8405245346869712\n"
     ]
    }
   ],
   "source": [
    "print_performance_metrics(lrCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f959c275-36a7-42a3-b1b7-551e28e651c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####GBM - CV Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cfc105-53b6-4690-b732-e6446b965087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.9042974614694501\naupr = 0.7736168393408911\nArea under PR = 0.6426837292019617\nArea under ROC = 0.7452119539184164\nAccuracy = 0.8477746942131771\nDenseMatrix([[6958.,  407.],\n             [1074., 1290.]])\nF1 of label 0 = 0.9038124309930505\nF1 of label 1 = 0.6353114996306329\nPrecision of label 0 = 0.8662848605577689\nPrecision of label 1 = 0.7601649970536241\nRecall of label 0 = 0.9447386286490156\nRecall of label 1 = 0.5456852791878173\nFPR of label 0 = 0.4543147208121827\nFPR of label 1 = 0.055261371350984384\nTPR of label 0 = 0.9447386286490156\nTPR of label 1 = 0.5456852791878173\n"
     ]
    }
   ],
   "source": [
    "print_performance_metrics(gbCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39417456-a688-48c7-9aa4-58801fbf147e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Logistic Regression - Model Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f567014e-5897-4b4f-b8a5-0aa1b536fe9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Intercept:  -5.9821391466705025\n"
     ]
    }
   ],
   "source": [
    "print('Model Intercept: ', lrCvModel.bestModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f8f9be-7672-4cee-83e1-3f72e632a07f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature Weight</th></tr></thead><tbody><tr><td>0.10728563291425162</td></tr><tr><td>-0.2961958701513941</td></tr><tr><td>-0.08240276890450877</td></tr><tr><td>-0.2781053241320957</td></tr><tr><td>-0.17741325409202524</td></tr><tr><td>0.3883038764029745</td></tr><tr><td>0.447508064612638</td></tr><tr><td>-1.8866287502815302</td></tr><tr><td>-0.19640038621592631</td></tr><tr><td>-0.0041308974494214746</td></tr><tr><td>0.3817931077161157</td></tr><tr><td>0.6284988311345274</td></tr><tr><td>0.09229260575747907</td></tr><tr><td>-0.550976358418968</td></tr><tr><td>-0.10623104529190863</td></tr><tr><td>-0.4890605013661405</td></tr><tr><td>-0.8678237436203432</td></tr><tr><td>0.9471353729749993</td></tr><tr><td>-0.6519605803614339</td></tr><tr><td>-0.3317698640034675</td></tr><tr><td>0.9584214854045416</td></tr><tr><td>-0.2573593596569983</td></tr><tr><td>-0.3433282171386037</td></tr><tr><td>0.6863392988415774</td></tr><tr><td>-0.6035635180249447</td></tr><tr><td>-0.24572383475327994</td></tr><tr><td>-0.2457249146256414</td></tr><tr><td>-0.01719191125818981</td></tr><tr><td>-0.4060146070152623</td></tr><tr><td>0.44010726177208725</td></tr><tr><td>-0.014673306215683685</td></tr><tr><td>0.6405969814782729</td></tr><tr><td>-0.06684124669001947</td></tr><tr><td>0.2139774602448999</td></tr><tr><td>-0.6523999067274737</td></tr><tr><td>-0.3864638791656345</td></tr><tr><td>-0.2777250611103521</td></tr><tr><td>-0.172481566272493</td></tr><tr><td>-0.5695455632302291</td></tr><tr><td>-0.7885204814319976</td></tr><tr><td>0.5208323170805699</td></tr><tr><td>0.403433660895344</td></tr><tr><td>-0.6974774212004072</td></tr><tr><td>0.5053398694433434</td></tr><tr><td>-0.1536681327777101</td></tr><tr><td>-0.62947040804748</td></tr><tr><td>-0.37060908655225616</td></tr><tr><td>1.213321473660665</td></tr><tr><td>0.12139678108007265</td></tr><tr><td>-0.11052394721073272</td></tr><tr><td>0.18959917600630574</td></tr><tr><td>-0.2056147250090643</td></tr><tr><td>0.46912587216278123</td></tr><tr><td>0.21774897067011148</td></tr><tr><td>-0.5689670307845954</td></tr><tr><td>-0.16761780865636497</td></tr><tr><td>0.3066464806374549</td></tr><tr><td>0.5156377462528039</td></tr><tr><td>0.5981711651635365</td></tr><tr><td>-0.13111117060004074</td></tr><tr><td>-0.4215656879297624</td></tr><tr><td>-0.46272012281937824</td></tr><tr><td>0.36064982941322826</td></tr><tr><td>0.18932527585751863</td></tr><tr><td>-0.3246423257408748</td></tr><tr><td>-0.4739278817236144</td></tr><tr><td>-0.4802453426575861</td></tr><tr><td>0.8413627153451679</td></tr><tr><td>-0.7691448588821359</td></tr><tr><td>-1.0109577315309095</td></tr><tr><td>0.31468989338844117</td></tr><tr><td>0.7877498592138686</td></tr><tr><td>-0.7078693061097616</td></tr><tr><td>-1.293521084648497</td></tr><tr><td>0.25517567269057917</td></tr><tr><td>0.18477689759510016</td></tr><tr><td>-0.35099401956277143</td></tr><tr><td>0.4959281085543585</td></tr><tr><td>-0.6052533131837212</td></tr><tr><td>-0.6340342309671774</td></tr><tr><td>-0.046670708144805485</td></tr><tr><td>-2.624536334458272</td></tr><tr><td>0.04025610542520033</td></tr><tr><td>-0.6461338580561099</td></tr><tr><td>0.1836682456172151</td></tr><tr><td>0.2726701934877572</td></tr><tr><td>-0.9658207075692551</td></tr><tr><td>-0.14150300845259572</td></tr><tr><td>0.20161147959179784</td></tr><tr><td>0.27886752962850864</td></tr><tr><td>-1.4496635819694923</td></tr><tr><td>-0.5535152520110531</td></tr><tr><td>0.6134917164991263</td></tr><tr><td>0.11840072805046425</td></tr><tr><td>0.020644038736542806</td></tr><tr><td>6.015316829290395E-7</td></tr><tr><td>0.12085956262993287</td></tr><tr><td>9.511010796712674E-5</td></tr><tr><td>5.486206539138475E-4</td></tr><tr><td>0.025059507568622463</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0.10728563291425162
        ],
        [
         -0.2961958701513941
        ],
        [
         -0.08240276890450877
        ],
        [
         -0.2781053241320957
        ],
        [
         -0.17741325409202524
        ],
        [
         0.3883038764029745
        ],
        [
         0.447508064612638
        ],
        [
         -1.8866287502815302
        ],
        [
         -0.19640038621592631
        ],
        [
         -0.0041308974494214746
        ],
        [
         0.3817931077161157
        ],
        [
         0.6284988311345274
        ],
        [
         0.09229260575747907
        ],
        [
         -0.550976358418968
        ],
        [
         -0.10623104529190863
        ],
        [
         -0.4890605013661405
        ],
        [
         -0.8678237436203432
        ],
        [
         0.9471353729749993
        ],
        [
         -0.6519605803614339
        ],
        [
         -0.3317698640034675
        ],
        [
         0.9584214854045416
        ],
        [
         -0.2573593596569983
        ],
        [
         -0.3433282171386037
        ],
        [
         0.6863392988415774
        ],
        [
         -0.6035635180249447
        ],
        [
         -0.24572383475327994
        ],
        [
         -0.2457249146256414
        ],
        [
         -0.01719191125818981
        ],
        [
         -0.4060146070152623
        ],
        [
         0.44010726177208725
        ],
        [
         -0.014673306215683685
        ],
        [
         0.6405969814782729
        ],
        [
         -0.06684124669001947
        ],
        [
         0.2139774602448999
        ],
        [
         -0.6523999067274737
        ],
        [
         -0.3864638791656345
        ],
        [
         -0.2777250611103521
        ],
        [
         -0.172481566272493
        ],
        [
         -0.5695455632302291
        ],
        [
         -0.7885204814319976
        ],
        [
         0.5208323170805699
        ],
        [
         0.403433660895344
        ],
        [
         -0.6974774212004072
        ],
        [
         0.5053398694433434
        ],
        [
         -0.1536681327777101
        ],
        [
         -0.62947040804748
        ],
        [
         -0.37060908655225616
        ],
        [
         1.213321473660665
        ],
        [
         0.12139678108007265
        ],
        [
         -0.11052394721073272
        ],
        [
         0.18959917600630574
        ],
        [
         -0.2056147250090643
        ],
        [
         0.46912587216278123
        ],
        [
         0.21774897067011148
        ],
        [
         -0.5689670307845954
        ],
        [
         -0.16761780865636497
        ],
        [
         0.3066464806374549
        ],
        [
         0.5156377462528039
        ],
        [
         0.5981711651635365
        ],
        [
         -0.13111117060004074
        ],
        [
         -0.4215656879297624
        ],
        [
         -0.46272012281937824
        ],
        [
         0.36064982941322826
        ],
        [
         0.18932527585751863
        ],
        [
         -0.3246423257408748
        ],
        [
         -0.4739278817236144
        ],
        [
         -0.4802453426575861
        ],
        [
         0.8413627153451679
        ],
        [
         -0.7691448588821359
        ],
        [
         -1.0109577315309095
        ],
        [
         0.31468989338844117
        ],
        [
         0.7877498592138686
        ],
        [
         -0.7078693061097616
        ],
        [
         -1.293521084648497
        ],
        [
         0.25517567269057917
        ],
        [
         0.18477689759510016
        ],
        [
         -0.35099401956277143
        ],
        [
         0.4959281085543585
        ],
        [
         -0.6052533131837212
        ],
        [
         -0.6340342309671774
        ],
        [
         -0.046670708144805485
        ],
        [
         -2.624536334458272
        ],
        [
         0.04025610542520033
        ],
        [
         -0.6461338580561099
        ],
        [
         0.1836682456172151
        ],
        [
         0.2726701934877572
        ],
        [
         -0.9658207075692551
        ],
        [
         -0.14150300845259572
        ],
        [
         0.20161147959179784
        ],
        [
         0.27886752962850864
        ],
        [
         -1.4496635819694923
        ],
        [
         -0.5535152520110531
        ],
        [
         0.6134917164991263
        ],
        [
         0.11840072805046425
        ],
        [
         0.020644038736542806
        ],
        [
         6.015316829290395E-7
        ],
        [
         0.12085956262993287
        ],
        [
         9.511010796712674E-5
        ],
        [
         5.486206539138475E-4
        ],
        [
         0.025059507568622463
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature Weight",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrWeights = lrCvModel.bestModel.coefficients\n",
    "lrWeights = [(float(w),) for w in lrWeights]  # convert numpy type to float, and to tuple\n",
    "lrWeightsDF = sqlContext.createDataFrame(lrWeights, [\"Feature Weight\"])\n",
    "display(lrWeightsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "857e8f3b-45fb-4af6-b8ce-dd30d3190095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03af6259-aa64-4309-a2a8-6207c8675e26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 8</span>\n",
    "1. Print out a table of feature_name and feature_coefficient from the Logistic Regression model.\n",
    "<br><br>\n",
    "Hint: Adapt the code from here: https://stackoverflow.com/questions/42935914/how-to-map-features-from-the-output-of-a-vectorassembler-back-to-the-column-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb525282-a308-446e-a14a-37fae0a9019d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Question 8.1 Answer ###\n",
    "\n",
    "# from: https://stackoverflow.com/questions/42935914/how-to-map-features-from-the-output-of-a-vectorassembler-back-to-the-column-name\n",
    "from itertools import chain\n",
    "\n",
    "transformed = lrCvModel.bestModel.transform(trainingData)\n",
    "attrs = sorted((attr[\"idx\"], attr[\"name\"]) for attr in (chain(*transformed.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a96224e-9d08-43e0-8035-727fa7112f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 'workclassclassVec_ Private'),\n",
       " (1, 'workclassclassVec_ Self-emp-not-inc'),\n",
       " (2, 'workclassclassVec_ Local-gov'),\n",
       " (3, 'workclassclassVec_ ?')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432159f9-6383-42eb-8ced-489f722d68e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  feature_name  feature_importance\n23  marital_statusclassVec_ Married-civ-spouse            0.223231\n94                                         age            0.205322\n99                              hours_per_week            0.143538\n96                               education_num            0.133469\n98                                capital_loss            0.083332\n..                                         ...                 ...\n42         occupationclassVec_ Priv-house-serv            0.000000\n41         occupationclassVec_ Protective-serv            0.000000\n40            occupationclassVec_ Tech-support            0.000000\n11                  educationclassVec_ Masters            0.000000\n50            raceclassVec_ Asian-Pac-Islander            0.000000\n\n[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "gbCvFeatureImportance = pd.DataFrame([(name, gbCvModel.bestModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\n",
    "\n",
    "print(gbCvFeatureImportance.sort_values(by=['feature_importance'],ascending =False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd47490f-2880-4b55-9b80-e74faa0a7fef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####<span style=\"color:darkblue\">Question 9</span>\n",
    "1. Build and train a RandomForestClassifier and print out a table of feature importances from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b683c493-7b73-43e1-b096-629ffdcb4a40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.8852794462766335\naupr = 0.7325342657968847\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:166: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.6387538400482328\nArea under ROC = 0.6624525727046222\nAccuracy = 0.8242368177613321\nDenseMatrix([[7197.,  168.],\n             [1542.,  822.]])\nF1 of label 0 = 0.8938152011922503\nF1 of label 1 = 0.49016100178890876\nPrecision of label 0 = 0.8235496052179884\nPrecision of label 1 = 0.8303030303030303\nRecall of label 0 = 0.9771894093686354\nRecall of label 1 = 0.3477157360406091\nFPR of label 0 = 0.6522842639593909\nFPR of label 1 = 0.022810590631364563\nTPR of label 0 = 0.9771894093686354\nTPR of label 1 = 0.3477157360406091\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol = 'label', featuresCol = 'features', numTrees = 10)\n",
    "\n",
    "rfModel = rf.fit(trainingData)\n",
    "\n",
    "rfPredictions = rfModel.transform(testData)\n",
    "\n",
    "print_performance_metrics(rfPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e069d0a-9338-434e-9d8f-087a860fc20f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: Whether bootstrap samples are used when building trees. (default: True)\ncacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\nlabelCol: label column name. (default: label, current: label)\nleafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\nmaxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\nmaxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\nminWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\nnumTrees: Number of trees to train (>= 1). (default: 20, current: 10)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nseed: random seed. (default: -5387697053847413545)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605ba0c1-21d1-47ad-a0fb-842ecea10b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-984230274225811>, line 15\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m rfCvModel \u001B[38;5;241m=\u001B[39m rfCv\u001B[38;5;241m.\u001B[39mfit(trainingData)\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# look at best params from the CV\u001B[39;00m\n",
       "\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28mprint\u001B[39m(rfCvModel\u001B[38;5;241m.\u001B[39mbestModel\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetimpurity())\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(rfCvModel\u001B[38;5;241m.\u001B[39mbestModel\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetmaxDepth())\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(rfCvModel\u001B[38;5;241m.\u001B[39mbestModel\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetnumTrees())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 188\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    190\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n",
       "\n",
       "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o56819.getimpurity. Trace:\n",
       "py4j.Py4JException: Method getimpurity([]) does not exist\n",
       "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:344)\n",
       "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:352)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:297)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-984230274225811>, line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m rfCvModel \u001B[38;5;241m=\u001B[39m rfCv\u001B[38;5;241m.\u001B[39mfit(trainingData)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# look at best params from the CV\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28mprint\u001B[39m(rfCvModel\u001B[38;5;241m.\u001B[39mbestModel\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetimpurity())\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(rfCvModel\u001B[38;5;241m.\u001B[39mbestModel\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetmaxDepth())\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(rfCvModel\u001B[38;5;241m.\u001B[39mbestModel\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mgetnumTrees())\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 188\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    190\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n\n\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o56819.getimpurity. Trace:\npy4j.Py4JException: Method getimpurity([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:344)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:352)\n\tat py4j.Gateway.invoke(Gateway.java:297)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
       "errorSummary": "py4j.Py4JException: Method getimpurity([]) does not exist",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "rfParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.impurity, ['gini', 'entropy'])\n",
    "             .addGrid(rf.maxDepth, [2, 3, 4])\n",
    "             .addGrid(rf.numTrees, [10, 20, 30])\n",
    "             .build())\n",
    "\n",
    "# Create CrossValidator\n",
    "rfCv = CrossValidator(estimator=rf, estimatorParamMaps=rfParamGrid, evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "rfCvModel = rfCv.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc164398-4373-4248-bfdb-2ef08a115612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini\n3\n30\n"
     ]
    }
   ],
   "source": [
    "# look at best params from the CV\n",
    "print(rfCvModel.bestModel._java_obj.getImpurity())\n",
    "print(rfCvModel.bestModel._java_obj.getMaxDepth())\n",
    "print(rfCvModel.bestModel._java_obj.getNumTrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7371d643-f85e-420a-8e0c-38305269326a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.8865384018939912\naupr = 0.7223034640971477\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:166: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.6396358217883891\nArea under ROC = 0.5458445763161613\nAccuracy = 0.7788056326446706\nDenseMatrix([[7.358e+03, 7.000e+00],\n             [2.145e+03, 2.190e+02]])\nF1 of label 0 = 0.872421152478065\nF1 of label 1 = 0.16911196911196913\nPrecision of label 0 = 0.774281805745554\nPrecision of label 1 = 0.9690265486725663\nRecall of label 0 = 0.9990495587236932\nRecall of label 1 = 0.09263959390862944\nFPR of label 0 = 0.9073604060913706\nFPR of label 1 = 0.0009504412763068567\nTPR of label 0 = 0.9990495587236932\nTPR of label 1 = 0.09263959390862944\n"
     ]
    }
   ],
   "source": [
    "# make predictions with best model\n",
    "rfCvPredictions = rfCvModel.transform(testData)\n",
    "\n",
    "# evaluate model performance\n",
    "print_performance_metrics(rfCvPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fca92d7-2518-40fe-9ce2-476afd80698a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  feature_name  feature_importance\n23  marital_statusclassVec_ Married-civ-spouse            0.223231\n94                                         age            0.205322\n99                              hours_per_week            0.143538\n96                               education_num            0.133469\n98                                capital_loss            0.083332\n..                                         ...                 ...\n42         occupationclassVec_ Priv-house-serv            0.000000\n41         occupationclassVec_ Protective-serv            0.000000\n40            occupationclassVec_ Tech-support            0.000000\n11                  educationclassVec_ Masters            0.000000\n50            raceclassVec_ Asian-Pac-Islander            0.000000\n\n[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "transformed = rfCvModel.bestModel.transform(trainingData)\n",
    "\n",
    "attrs = sorted((attr[\"idx\"], attr[\"name\"]) for attr in (chain(*transformed.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"].values())))\n",
    "\n",
    "rfCvFeatureImportance = pd.DataFrame([(name, gbCvModel.bestModel.featureImportances[idx]) for idx, name in attrs],\n",
    "                                     columns=['feature_name','feature_importance'])\n",
    "\n",
    "print(rfCvFeatureImportance.sort_values(by=['feature_importance'], ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d63f4af-4566-4657-8f7e-a6b848f03356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  feature_name  feature_importance\n43               relationshipclassVec_ Husband            0.184534\n94                                         age            0.125365\n97                                capital_gain            0.122973\n23  marital_statusclassVec_ Married-civ-spouse            0.103903\n24       marital_statusclassVec_ Never-married            0.098588\n..                                         ...                 ...\n65               native_countryclassVec_ South            0.000000\n14               educationclassVec_ Assoc-acdm            0.000000\n67               native_countryclassVec_ Italy            0.000000\n13                     educationclassVec_ 11th            0.000000\n68  native_countryclassVec_ Dominican-Republic            0.000000\n\n[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "### Question 9.1 Answer ###\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "rfModel = rf.fit(trainingData)\n",
    "\n",
    "transformed = rfModel.transform(trainingData)\n",
    "attrs = sorted((attr[\"idx\"], attr[\"name\"]) for attr in (chain(*transformed.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"].values())))\n",
    "rfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs], columns=['feature_name','feature_importance'])\n",
    "print(rfFeatureImportance.sort_values(by=['feature_importance'],ascending=False))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2826965200986551,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Spark: Databricks Edition mini project FWang",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
